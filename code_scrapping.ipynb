{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping smartphone...\n",
      "‚úÖ Amazon : Donn√©es r√©cup√©r√©es pour smartphone.\n",
      "‚úÖ AliExpress : Donn√©es r√©cup√©r√©es pour smartphone.\n",
      "‚úÖ Boulanger : Donn√©es r√©cup√©r√©es pour smartphone.\n",
      "‚úÖ LDLC : Donn√©es r√©cup√©r√©es pour smartphone.\n",
      "‚úÖ Materiel.net : Donn√©es r√©cup√©r√©es pour smartphone.\n",
      "‚úÖ Banggood : Donn√©es r√©cup√©r√©es pour smartphone.\n",
      "Scraping Smartwatch...\n",
      "‚úÖ Amazon : Donn√©es r√©cup√©r√©es pour Smartwatch.\n",
      "‚úÖ AliExpress : Donn√©es r√©cup√©r√©es pour Smartwatch.\n",
      "‚úÖ Boulanger : Donn√©es r√©cup√©r√©es pour Smartwatch.\n",
      "‚úÖ LDLC : Donn√©es r√©cup√©r√©es pour Smartwatch.\n",
      "‚úÖ Materiel.net : Donn√©es r√©cup√©r√©es pour Smartwatch.\n",
      "‚úÖ Banggood : Donn√©es r√©cup√©r√©es pour Smartwatch.\n",
      "Scraping ecouteurs...\n",
      "‚úÖ Amazon : Donn√©es r√©cup√©r√©es pour ecouteurs.\n",
      "‚úÖ AliExpress : Donn√©es r√©cup√©r√©es pour ecouteurs.\n",
      "‚úÖ Boulanger : Donn√©es r√©cup√©r√©es pour ecouteurs.\n",
      "‚úÖ LDLC : Donn√©es r√©cup√©r√©es pour ecouteurs.\n",
      "‚úÖ Materiel.net : Donn√©es r√©cup√©r√©es pour ecouteurs.\n",
      "‚úÖ Banggood : Donn√©es r√©cup√©r√©es pour ecouteurs.\n",
      "Scraping PC...\n",
      "‚úÖ Amazon : Donn√©es r√©cup√©r√©es pour PC.\n",
      "‚úÖ AliExpress : Donn√©es r√©cup√©r√©es pour PC.\n",
      "‚úÖ Boulanger : Donn√©es r√©cup√©r√©es pour PC.\n",
      "‚úÖ LDLC : Donn√©es r√©cup√©r√©es pour PC.\n",
      "‚úÖ Materiel.net : Donn√©es r√©cup√©r√©es pour PC.\n",
      "‚úÖ Banggood : Donn√©es r√©cup√©r√©es pour PC.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# D√©finition de l'User-Agent pour √©viter le blocage\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Liste des produits √† scraper\n",
    "products = ['smartphone', 'Smartwatch', 'ecouteurs', 'PC']\n",
    "\n",
    "# URLs des sites de vente en ligne\n",
    "base_urls = {\n",
    "    'Amazon': 'https://www.amazon.fr/s?k={}',\n",
    "    'AliExpress': 'https://www.aliexpress.com/wholesale?SearchText={}',\n",
    "    'Boulanger': 'https://www.boulanger.com/resultats?tr={}',\n",
    "    'LDLC': 'https://www.ldlc.com/recherche/{}',\n",
    "    'Materiel.net': 'https://www.materiel.net/recherche/{}',\n",
    "    'Banggood': 'https://www.banggood.com/search/{}.html'\n",
    "}\n",
    "\n",
    "# Fonction de scraping\n",
    "def scrape_product(product):\n",
    "    print(f\"Scraping {product}...\")\n",
    "    for site, base_url in base_urls.items():\n",
    "        url = base_url.format(product.replace(' ', '+'))\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()  # V√©rification des erreurs HTTP\n",
    "            print(f\"‚úÖ {site} : Donn√©es r√©cup√©r√©es pour {product}.\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ùå Erreur lors de la requ√™te {site} : {e}\")\n",
    "\n",
    "        # Pause pour √©viter le blocage par les sites\n",
    "        time.sleep(2)\n",
    "\n",
    "# Ex√©cution du scraping pour tous les produits\n",
    "for product in products:\n",
    "    scrape_product(product)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä R√©sultats du scraping :\n",
      "      Produit       Site                                              Titre  \\\n",
      "0  smartphone  Boulanger  Smartphone\\n\\t\\t\\t\\t\\t\\t\\tSAMSUNG\\n\\t\\t\\t\\t\\t\\...   \n",
      "1  smartwatch  Boulanger  Montre connect√©e\\n\\t\\t\\t\\t\\t\\t\\tSAMSUNG\\n\\t\\t\\...   \n",
      "2   ecouteurs  Boulanger  Ecouteurs\\n\\t\\t\\t\\t\\t\\t\\tSAMSUNG\\n\\t\\t\\t\\t\\t\\t...   \n",
      "3          PC  Boulanger  PC Gamer\\n\\t\\t\\t\\t\\t\\t\\tASUS\\n\\t\\t\\t\\t\\t\\t\\tTU...   \n",
      "\n",
      "   Prix (‚Ç¨)               Note    Avis         Stock R√©f√©rence  \n",
      "0   421.99‚Ç¨                5.0   [(1)]       381,99‚Ç¨       N/A  \n",
      "1   699.99‚Ç¨  4.622000217437744  [(82)]       569,00‚Ç¨       N/A  \n",
      "2    59.99‚Ç¨  4.684199810028076  [(19)]  Indisponible       N/A  \n",
      "3  1049.99‚Ç¨                5.0   [(6)]  Indisponible       N/A  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from parsel import Selector\n",
    "\n",
    "# D√©finition de l'User-Agent\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Liste des sites √† scraper\n",
    "base_urls = {\n",
    "    'Boulanger': 'https://www.boulanger.com/resultats?tr={}',\n",
    "}\n",
    "\n",
    "# Liste des produits √† scraper\n",
    "products = ['smartphone', 'smartwatch', 'ecouteurs', 'PC']\n",
    "\n",
    "# Stockage des r√©sultats\n",
    "all_data = []\n",
    "\n",
    "# Fonction de scraping\n",
    "def scrape_product(site, url, product):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()  # V√©rification des erreurs HTTP\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#\\39 3e9ff98-ddd5-4940-ad69-a22ec4f875e6 > div > div > div > div > span > div > div > div.a-section.a-spacing-small.puis-padding-left-small.puis-padding-right-small > div.a-section.a-spacing-none.a-spacing-top-small.s-price-instructions-style\n",
    "        # Extraction des donn√©es selon le site\n",
    "        if site == \"Boulanger\":\n",
    "            items = soup.select('#product-list > ul > li > article')\n",
    "            for item in items:\n",
    "                title = item.select_one('#productLabel')\n",
    "                price= item.select_one('div.product-list__product-area-3.g-col-5.g-col-sm-7.g-col-md-4.g-start-md-9.g-col-lg-3.g-start-lg-7.g-col-xl-3.g-start-xl-7 > div > p')\n",
    "                rating = item.select_one('div.product-list__product-area-2.g-col-5.g-col-sm-7.g-col-md-4.g-col-lg-3.g-col-xl-3 > div.product-list__product-rating > a > div > bl-rating').get('rating')\n",
    "                reviews = item.select_one('div.product-list__product-area-2.g-col-5.g-col-sm-7.g-col-md-4.g-col-lg-3.g-col-xl-3 > div.product-list__product-rating > a > span')\n",
    "                stock = item.select_one('div.product-list__product-area-3.g-col-5.g-col-sm-7.g-col-md-4.g-start-md-9.g-col-lg-3.g-start-lg-7.g-col-xl-3.g-start-xl-7 > div > div > span.price__crossed')\n",
    "                reference = item.get(\"data-product-id\", \"N/A\")\n",
    "\n",
    "                if title and price:\n",
    "                    all_data.append({\n",
    "                        'Produit': product,\n",
    "                        'Site': site,\n",
    "                        'Titre': title.text.strip(),\n",
    "                        'Prix (‚Ç¨)': price.text.replace(',', '.').strip() if price else None,\n",
    "                        'Note': rating if rating else None,\n",
    "                        'Avis': reviews if reviews else None,  # Boulanger ne montre pas toujours le nombre d‚Äôavis\n",
    "                        'promo': stock.text.strip() if stock else \"Indisponible\",\n",
    "                        'R√©f√©rence': reference\n",
    "                    })\n",
    "\n",
    "        # Ajout d'autres sites ici (LDLC, Materiel.net, Banggood) en suivant la m√™me logique\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Erreur lors de la requ√™te {site}: {e}\")\n",
    "\n",
    "    time.sleep(2)  # Pause pour √©viter le blocage\n",
    "\n",
    "# Ex√©cution du scraping\n",
    "for product in products:\n",
    "    for site, base_url in base_urls.items():\n",
    "        scrape_product(site, base_url.format(product.replace(' ', '+')), product)\n",
    "\n",
    "# Conversion en DataFrame Pandas pour visualisation\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Affichage des r√©sultats\n",
    "print(\"\\nüìä R√©sultats du scraping :\")\n",
    "print(df)\n",
    "\n",
    "# Sauvegarde en CSV\n",
    "df.to_csv(\"resultats_scrapingboul.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä R√©sultats du scraping :\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.edge.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "# Configuration de Selenium avec Edgefrom selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service as EdgeService\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "edge_options = Options()\n",
    "edge_options.add_argument(\"--headless\")  # Ex√©cuter en mode headless (sans interface graphique)\n",
    "edge_options.add_argument(\"--disable-gpu\")\n",
    "edge_options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "# Chemin vers le pilote Edge (Microsoft Edge WebDriver)\n",
    "edge_service = Service(executable_path=\"msedgedriver.exe\")  # Remplacez par le chemin de votre WebDriver\n",
    "\n",
    "# Initialisation du navigateur Edge\n",
    "loading_time = 30\n",
    "\n",
    "driver = webdriver.Edge(service=EdgeService(EdgeChromiumDriverManager().install()))\n",
    "   \n",
    "\n",
    "# Liste des URLs √† scraper\n",
    "urls = [\n",
    "    'https://www.boulanger.com/c/montre-connectee',\n",
    "    'https://www.boulanger.com/c/airpods',\n",
    "    'https://www.boulanger.com/c/tous-les-ordinateurs-portables',\n",
    "    'https://www.boulanger.com/c/smartphone-telephone-portable'\n",
    "]\n",
    "\n",
    "# Liste des produits √† scraper\n",
    "products = ['smartwatch', 'ecouteurs', 'PC', 'smartphone']\n",
    "\n",
    "# Stockage des r√©sultats\n",
    "all_data = []\n",
    "\n",
    "# Fonction de scraping avec Selenium\n",
    "def scrape_product(url, product):\n",
    "    try:\n",
    "        # Charger la page\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # Attendre que la page se charge compl√®tement\n",
    "\n",
    "        # Extraire les √©l√©ments de la page\n",
    "        items = driver.find_elements(By.CSS_SELECTOR, '#product-list > ul > li > article')  # S√©lecteur principal pour les produits\n",
    "        \n",
    "        for item in items:\n",
    "            try:\n",
    "                title = item.find_element(By.CSS_SELECTOR, '#productLabel').text  # Titre du produit\n",
    "            except NoSuchElementException:\n",
    "                title = None\n",
    "\n",
    "            try:\n",
    "                price = item.find_element(By.CSS_SELECTOR, '.product-list__product-area-3.g-col-5.g-col-sm-7.g-col-md-4.g-start-md-9.g-col-lg-3.g-start-lg-7.g-col-xl-3.g-start-xl-7 > div > p').text  # Prix\n",
    "            except NoSuchElementException:\n",
    "                price = None\n",
    "\n",
    "            try:\n",
    "                rating = item.find_element(By.CSS_SELECTOR, 'bl-rating').get_attribute('rating')  # Note\n",
    "            except NoSuchElementException:\n",
    "                rating = None\n",
    "\n",
    "            try:\n",
    "                reviews = item.find_element(By.CSS_SELECTOR, '.product-list__product-area-2.g-col-5.g-col-sm-7.g-col-md-4.g-col-lg-3.g-col-xl-3 > div.product-list__product-rating > a > span').text  # Avis\n",
    "            except NoSuchElementException:\n",
    "                reviews = None\n",
    "\n",
    "            try:\n",
    "                promo = item.find_element(By.CSS_SELECTOR, '.product-list__product-area-3.g-col-5.g-col-sm-7.g-col-md-4.g-start-md-9.g-col-lg-3.g-start-lg-7.g-col-xl-3.g-start-xl-7 > div > div > span.price__crossed').text  # Stock ou promo\n",
    "            except NoSuchElementException:\n",
    "                promo = \"rien\"\n",
    "            try:\n",
    "                stock = item.find_element(By.CSS_SELECTOR, '.product-list__product-area-3.g-col-5.g-col-sm-7.g-col-md-4.g-start-md-9.g-col-lg-3.g-start-lg-7.g-col-xl-3.g-start-xl-7 > button > span').text  # Stock ou promo\n",
    "            except NoSuchElementException:\n",
    "                stock = \"Indisponible\"\n",
    "\n",
    "            # Ajouter les donn√©es extraites √† la liste\n",
    "            if title and price:\n",
    "                all_data.append({\n",
    "                    'Produit': product,\n",
    "                    'Site': 'Boulanger',\n",
    "                    'Titre': title.strip(),\n",
    "                    'Prix (‚Ç¨)': price.replace('‚Ç¨', '').strip() if price else None,\n",
    "                    'Note': rating if rating else None,\n",
    "                    'Avis': reviews.strip() if reviews else None,\n",
    "                    'Promo': promo.replace('‚Ç¨', '').strip() if promo else None,\n",
    "                    'Stock': stock.strip() if stock else \"Indisponible\"\n",
    "                })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors du scraping de {url} : {e}\")\n",
    "\n",
    "# Ex√©cution du scraping\n",
    "for product, url in zip(products, urls):\n",
    "    scrape_product(url, product)\n",
    "\n",
    "# Fermer le navigateur\n",
    "driver.quit()\n",
    "\n",
    "# Conversion en DataFrame Pandas pour visualisation\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Affichage des r√©sultats\n",
    "print(\"\\nüìä R√©sultats du scraping :\")\n",
    "print(df)\n",
    "\n",
    "# Sauvegarde en CSV\n",
    "df.to_csv(\"resultats_scraping_boulanger_selenium.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraping_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
